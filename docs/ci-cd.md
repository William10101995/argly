# Pipeline CI/CD

Diagrama del flujo de integraciÃ³n y despliegue continuo.

```mermaid
flowchart LR
    subgraph TRIGGER["Trigger"]
        CRON["â° Cron Schedule"]
        MANUAL["ðŸ–±ï¸ Manual Dispatch"]
    end

    subgraph GHA["GitHub Actions"]
        CHECKOUT["ðŸ“¥ Checkout cÃ³digo"]
        SETUP["ðŸ Setup Python 3.12"]
        DEPS["ðŸ“¦ pip install"]
        SCRAPER["ðŸ•·ï¸ Ejecutar scraper"]
        COMMIT["ðŸ’¾ Git commit"]
        PUSH["ðŸš€ Git push"]
    end

    subgraph VERCEL["Vercel"]
        DETECT["ðŸ” Detectar cambios"]
        BUILD["ðŸ—ï¸ Build"]
        DEPLOY["ðŸŒ Deploy"]
    end

    PROD["âœ… api.argly.com.ar"]

    CRON --> CHECKOUT
    MANUAL --> CHECKOUT
    CHECKOUT --> SETUP
    SETUP --> DEPS
    DEPS --> SCRAPER
    SCRAPER --> |data/*.json| COMMIT
    COMMIT --> PUSH
    PUSH --> |webhook| DETECT
    DETECT --> BUILD
    BUILD --> DEPLOY
    DEPLOY --> PROD
```

## Workflows Configurados

### combustibles.yml
```yaml
schedule:
  - cron: '0 6 1,16 * *'  # DÃ­as 1 y 16 a las 06:00 UTC
```

### icl.yml
```yaml
schedule:
  - cron: '0 12,13,14,15 * * *'  # 12:00, 13:00, 14:00, 15:00 UTC
```

### ipc.yml
```yaml
schedule:
  - cron: '0 5 10-14 * *'  # DÃ­as 10-14 a las 05:00 UTC
```

## Flujo de Commit AutomÃ¡tico

```mermaid
sequenceDiagram
    participant GHA as GitHub Actions
    participant Scraper
    participant Git
    participant Vercel

    GHA->>Scraper: Ejecutar script
    Scraper->>Scraper: Fetch datos pÃºblicos
    Scraper->>Scraper: Parse HTML
    Scraper->>Git: Guardar JSON en /data

    alt Hay cambios en data/
        Git->>Git: git add data/
        Git->>Git: git commit -m "chore: update {dataset}"
        Git->>Git: git push origin main
        Git-->>Vercel: Webhook trigger
        Vercel->>Vercel: Build & Deploy
    else Sin cambios
        Git-->>GHA: Skip commit
    end
```
